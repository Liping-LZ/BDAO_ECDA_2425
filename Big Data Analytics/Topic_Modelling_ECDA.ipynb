{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2JVE6b2tnILV5gnyVNh0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liping-LZ/BDAO_ECDA_2425/blob/main/Big%20Data%20Analytics/Topic_Modelling_ECDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is Natural Language Processing (NLP)**\n",
        "\n",
        "Natural Language Processing (NLP) is broadly defined as the automatic manipulation of natural language, like speech and text, by software. In other words, NLP is an important technique to help understand human language. NLP is a broad topic but we are mainly talking about how to use NLP techniques to do text mining and text analysis. In this tutorial, we will talk about text cleaning and data text processing, which are the essential steps to get data prepared for further text mining.\n"
      ],
      "metadata": {
        "id": "tX_AGVES0KEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modelling**\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a classic model to do topic modelling. Topic modeling is unsupervised learning and the goal is to group different documents to the same “topic”."
      ],
      "metadata": {
        "id": "7vmwZEW9_PUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. import data you would like to analyse**"
      ],
      "metadata": {
        "id": "CCwhAcbc_7NO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLAdb7rY-qUG"
      },
      "outputs": [],
      "source": [
        "# Let's import data first\n",
        "# Run the code and upload the csv file from your laptop\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = 'you file name' # change the csv file name to your file name that you uploaded\n",
        "df = pd.read_csv(data)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YDl5IA5VMHtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = 'the review column name' #change the column name to where the review is"
      ],
      "metadata": {
        "id": "MIVF0TNWAu9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Text cleaning & Text processing**\n",
        "Usually, we need to do some proper text cleanning and processing before text mining (e.g. topic modeling). Here are the common steps to follow:\n",
        "\n",
        "#### **Step 1: Cleaning text**\n",
        "- (optional) Encoding. Usually you don't need to set up the encoding type or just set it as \"utf-8\" (which is suitable for all languages), but sometimes if your text data is non-English, you might need to look for the right encoding type. For example, \"latin_1\" is suitable for the languages in Western Europe and \"gbk\" for Chinese. Here is the [List of Python standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).\n",
        "- Lower casing;\n",
        "- Remove special characters, such as emoji;\n",
        "- Remove email address and url;\n",
        "- Remove punctuation\n",
        "\n",
        "#### **Step 2: Tokenisation**\n",
        "In this step, the text is split into smaller units. Sentence-->words.\n",
        "\n",
        "#### **Step 3: Remove stop words**\n",
        "stop words are a set of commonly used words in a language. Example of stop words in English are \"is\", \"a\", \"the\" and etc. These words are usually not useful, so we normally remove them.\n",
        "\n",
        "#### **Step 4: Stemming or Lemmatisation**\n",
        "Stemming is the text standardization step where the words are stemmed or diminished to their root/base form. For example, words like ‘programmer’, ‘programming, ‘program’ will be stemmed to ‘program’. But the disadvantage of stemming is that it stems the words such that its root form loses the meaning or it is not diminished to a proper English word. For example, \"manages\" will be stemmed to \"manag\".\n",
        "\n",
        "Lemmatisation also stems the words but try to make sure the words are not losing their meaning.Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary while diminishing. Thus, the words make more sense in this case, but lemmatisation might take longer to run.\n",
        "\n",
        "We don't need to use both, but which one to choose? It depends. Sometimes stemming works fine then it's more effective. But if we need to get the actual meaning with actual words, then lemmatisation is more suitable.\n",
        "\n",
        "#### **Step 5: Once the processing are done, put the tokens back together as text**"
      ],
      "metadata": {
        "id": "FwEqMOqvCeKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# drop data with missing values in the 'content' column\n",
        "df = df.dropna(subset=[target_column])\n",
        "\n",
        "# drop duplicate review content\n",
        "df = df.drop_duplicates(subset=[target_column])\n",
        "\n",
        "# remove contraction\n",
        "df[target_column] = df[target_column].map(lambda x: contractions.fix(x))\n",
        "\n",
        "# convert the relevant column to lowercase\n",
        "df[target_column] = df[target_column].str.lower()\n",
        "\n",
        "# Remove overspace\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('\\s{2,}', \" \", x))\n",
        "\n",
        "# Remove non-word characters, so numbers and ___ etc\n",
        "df[target_column] = df[target_column].str.replace(\"[^A-Za-z]\", \" \", regex = True)\n",
        "\n",
        "# Remove url link\n",
        "df[target_column] = df[target_column].apply(lambda x: re.sub('http://\\S+|https://\\S+', '', x))\n",
        "\n",
        "# Remove email address\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('\\S*@\\S*\\s?', '', x))\n",
        "\n",
        "# Remove punctuation\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n",
        "\n",
        "# create a list of the contents from the 'contents' column\n",
        "words = df[target_column].tolist()\n",
        "\n",
        "# tokenise the words\n",
        "word_tokens = []\n",
        "for content in words:\n",
        "    word_tokens.append(word_tokenize(content))\n",
        "\n",
        "# create bigram model\n",
        "bigram = gensim.models.phrases.Phrases(word_tokens, min_count=3, threshold=10)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram) # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "\n",
        "# NLTK Stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['flight','british','airway']) #add more stopwords here\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Define functions for stopwords, bigrams and lemmatisation\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "# Remove stopwords\n",
        "data_words_nostops = remove_stopwords(word_tokens)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Do lemmatisation keeping only noun, adj, vb, adv\n",
        "data_lemmatised = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "# put the tokens back together as text to have our filtered contents\n",
        "\n",
        "rejoin = []\n",
        "for content in data_lemmatised: # Here we choose to use stemming instead of lemmatisation\n",
        "    x = \" \".join(content) # join the text back together\n",
        "    rejoin.append(x)\n",
        "\n",
        "# add the reformed text to the data frame\n",
        "df['cleaned_review'] = rejoin"
      ],
      "metadata": {
        "id": "eEnsrDs8A2eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Build the LDA model**"
      ],
      "metadata": {
        "id": "g_MX7WkJJN-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorise the data into word counts\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "max_words = 1000 #how many words taking account for topic modeling\n",
        "vectorizer = CountVectorizer(max_features=max_words)\n",
        "vec = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n",
        "k = 4 #this is the number of the topic. you can decide the number\n",
        "\n",
        "lda = LDA(n_components=k, max_iter=5, learning_method='online', random_state = 10)\n",
        "lda.fit(vec)"
      ],
      "metadata": {
        "id": "L0SAqJIzDuba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Visualisation of the topics**"
      ],
      "metadata": {
        "id": "-n28wuX2JXq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud\n",
        "\n",
        "#declaring number of terms we need per topic\n",
        "terms_count = 50\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "wcloud = wordcloud.WordCloud(background_color=\"White\",mask=None, max_words=100,\\\n",
        "                             max_font_size=60,min_font_size=10,prefer_horizontal=0.9,\n",
        "                             contour_width=3,contour_color='Black',colormap='Set2')\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(30, 15), sharex=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx,topic in enumerate(lda.components_):\n",
        "    print('Topic# ',idx+1)\n",
        "    abs_topic = abs(topic)\n",
        "    topic_terms = [[terms[i],topic[i]] for i in abs_topic.argsort()[:-terms_count-1:-1]]\n",
        "    topic_terms_sorted = [[terms[i], topic[i]] for i in abs_topic.argsort()[:-terms_count - 1:-1]]\n",
        "    topic_words = []\n",
        "    for i in range(terms_count):\n",
        "        topic_words.append(topic_terms_sorted[i][0])\n",
        "    print(','.join( word for word in topic_words))\n",
        "    print(\"\")\n",
        "    dict_word_frequency = {}\n",
        "\n",
        "    for i in range(terms_count):\n",
        "        dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]\n",
        "\n",
        "    ax = axes[idx]\n",
        "    ax.set_title(f'Topic {idx +1}',fontdict={'fontsize': 30})\n",
        "    wcloud.generate_from_frequencies(dict_word_frequency)\n",
        "    ax.imshow(wcloud, interpolation='bilinear')\n",
        "    ax.axis(\"off\")"
      ],
      "metadata": {
        "id": "elXstxIYGzct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the result into bar charts in topic\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# helper function to plot topics\n",
        "# see Grisel et al.\n",
        "# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
        "def plot_top_words(model, feature_names, n_top_words, title):\n",
        "    fig, axes = plt.subplots(1, 6, figsize=(30, 15), sharex=True)\n",
        "    axes = axes.flatten()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
        "        top_features = [feature_names[i] for i in top_features_ind]\n",
        "        weights = topic[top_features_ind]\n",
        "\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_features, weights, height=0.7)\n",
        "        ax.set_title(f'Topic {topic_idx +1}',\n",
        "                     fontdict={'fontsize': 30})\n",
        "        ax.invert_yaxis()\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "        for i in 'top right left'.split():\n",
        "            ax.spines[i].set_visible(False)\n",
        "        fig.suptitle(title, fontsize=40)\n",
        "\n",
        "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "    plt.show()\n",
        "\n",
        "n_top_words = 20  #how many words to be visualised in each topic\n",
        "\n",
        "# get the list of words (feature names)\n",
        "vec_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# print the top words per topic\n",
        "plot_top_words(lda, vec_feature_names, n_top_words, 'Topics in LDA model')"
      ],
      "metadata": {
        "id": "dKV08bKSEoYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Download file with assigned topic**"
      ],
      "metadata": {
        "id": "UpRn6KhZg1xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "doc_topic = lda.transform(vec)\n",
        "docsVStopics = pd.DataFrame(doc_topic, columns=[\"Topic\"+str(i+1) for i in range(k)])\n",
        "df = df.join(docsVStopics)\n",
        "df['mostlikely_topic'] = docsVStopics.idxmax(axis=1)\n",
        "\n",
        "df.to_csv('topic_modeling_result.csv', index=False) # save the file to google drive\n",
        "files.download('topic_modeling_result.csv') # download the file to your local machine"
      ],
      "metadata": {
        "id": "SXEQdUAVgtmY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}